{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "682f9f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This validation code serves the following purposes\n",
    "#1. perform a sanity check. binary search method and Ben-tal's method should return the same gradient and objective value.\n",
    "#2. check that when Gamma<3, outputs from binary search method are optimal, by checking KKT conditions (grad is a multiple of np.ones(n))\n",
    "#3. show that when Gamma<3, Dankins' Theorem gradient match finite central diff gradient. \n",
    "#4. show that at termination of Frank Wolfe, we get the same weights by using bisection or Ben-tal's method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "885861f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preparation done\n"
     ]
    }
   ],
   "source": [
    "from gurobipy import *\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from scipy.optimize import minimize\n",
    "import time\n",
    "import scipy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LassoLarsCV\n",
    "import statistics\n",
    "import time\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.utils.random import sample_without_replacement\n",
    "\n",
    "t = time.time()\n",
    "#parameters\n",
    "priceMultiplier=1#multiplier of price in kernel\n",
    "Gamma=0.05#100\n",
    "P_low=5\n",
    "P_high=10\n",
    "\n",
    "sigma=1#bandwidth of kernel\n",
    "dim=1#dim of feature X\n",
    "\n",
    "def policy_to_evaluate(x,p):\n",
    "    return 1.1*p\n",
    "\n",
    "def expected_demand(x_arr,p_arr):\n",
    "    #d_arr=(x_arr*7-p_arr+P_high)/(P_high-P_low+7)\n",
    "    d_arr=(5*x_arr+3*(P_high-p_arr)-10)/10\n",
    "    d_arr=np.minimum(d_arr,1)\n",
    "    d_arr=np.maximum(d_arr,0)\n",
    "    return d_arr\n",
    "\n",
    "def realized_demand(x_arr,p_arr,rndseed):\n",
    "    np.random.seed(rndseed+52028)\n",
    "    return np.random.binomial(1,expected_demand(x_arr,p_arr))\n",
    "    \n",
    "#generate training dataset\n",
    "rndseed=927#2352\n",
    "train_size=30\n",
    "np.random.seed(rndseed+883)\n",
    "X_train=np.random.uniform(0,1,train_size)\n",
    "#np.random.seed(rndseed+5325)\n",
    "#P_train=np.random.uniform(P_low,P_high,train_size)\n",
    "P_train=P_high-(P_high-P_low)*X_train#deterministic pricing\n",
    "D_train=realized_demand(X_train,P_train,rndseed+917)\n",
    "R_train=np.multiply(D_train,P_train)\n",
    "expected_R_train=np.multiply(P_train,expected_demand(X_train,P_train))\n",
    "\n",
    "#generate counterfactuals\n",
    "P_test=np.zeros(train_size)\n",
    "for i in range(0,train_size):\n",
    "    P_test[i]=policy_to_evaluate(X_train[i],P_train[i])\n",
    "D_test=realized_demand(X_train,P_test,rndseed+1847)\n",
    "realized_val=D_test@P_test/train_size\n",
    "expected_R_test=np.multiply(P_test,expected_demand(X_train,P_test))\n",
    "expected_val_test=sum(expected_R_test)/train_size\n",
    "\n",
    "#for computing kernel only: normalize prices, and multiply by given factor\n",
    "P_train_norm=priceMultiplier*(P_train-P_low)/ (P_high-P_low)\n",
    "\n",
    "Z_train=np.zeros((train_size,dim+1))\n",
    "for i in range(0,train_size):\n",
    "    Z_train[i]=np.append(X_train[i],P_train_norm[i])#historical policy\n",
    "Y_train=np.zeros((train_size,dim+1))\n",
    "for i in range(0,train_size):\n",
    "    Y_train[i]=np.append(X_train[i],policy_to_evaluate(X_train[i],P_train_norm[i]))#new policy\n",
    "\n",
    "#gram matrix\n",
    "kernel = RBF(sigma)\n",
    "Z_and_Y=np.zeros((train_size*2,dim+1))\n",
    "Z_and_Y[0:train_size,:]=Z_train\n",
    "Z_and_Y[train_size:2*train_size,:]=Y_train\n",
    "G=kernel(Z_and_Y)\n",
    "G=G+0.0001*np.identity(train_size*2)#make G positive semi-definite\n",
    "C=np.linalg.cholesky(G*2)#C is lower triangular, CC^\\top=G\n",
    "\n",
    "#preparation\n",
    "rowSumHalfG=np.transpose(np.sum(G[:,train_size:2*train_size],axis=1).reshape(-1,1))#sum each row of right half of G\n",
    "D3=np.transpose(rowSumHalfG).dot(rowSumHalfG)/(train_size**2)#the third term in Dw\n",
    "arr_m=np.zeros((train_size,train_size*2,train_size*2))\n",
    "for i in range(0,train_size):\n",
    "    temp2=G[:,i].reshape(-1,1)\n",
    "    arr_m[i]=temp2.dot(np.transpose(temp2))\n",
    "\n",
    "    \n",
    "print('preparation done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e526f4a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Academic license - for non-commercial use only - expires 2022-09-08\n",
      "Using license file C:\\Users\\yunfan\\gurobi.lic\n"
     ]
    }
   ],
   "source": [
    "#Nathan's method\n",
    "def nathans_method(lamda): \n",
    "    G_aug=np.copy(G)\n",
    "    for i in range(0,train_size):\n",
    "        G_aug[i][i]=G_aug[i][i]+lamda**2#add variance term\n",
    "\n",
    "    #evals and evectors of G\n",
    "    evals,evecs=scipy.linalg.eigh(G_aug, eigvals_only=False)#this method assumes real symmetric matrix, and is fast\n",
    "    A=np.diag(evals)\n",
    "    c=np.sum(evecs[0:train_size,0:2*train_size],axis=0)\n",
    "\n",
    "    #build optimization model\n",
    "    m = Model()\n",
    "    m.Params.LogToConsole = 0#suppress Gurobipy printing\n",
    "    # Add variables\n",
    "    v=m.addMVar ( train_size*2, ub=1.0,lb=-1.0,name=\"v\" )\n",
    "    # Set objective function\n",
    "    #m.Params.OptimalityTol=0.01\n",
    "    m.setObjective(v@A@v, GRB.MINIMIZE) \n",
    "    # Add constraints\n",
    "    m.addConstr(v@c==1)\n",
    "    m.addConstr(evecs[train_size:2*train_size,0:2*train_size]@v==-np.ones(train_size)/train_size)\n",
    "    #m.addConstr(S[0:train_size,0:2*train_size]@v>=-np.zeros(train_size))#w_i>=0\n",
    "    #m.params.method=0#0 is primal simplex, 1 is dual simplex, 2 is barrier\n",
    "    m.update()\n",
    "    m.optimize()\n",
    "\n",
    "    #print key info for testing purpose\n",
    "    v_star=np.zeros(2*train_size)\n",
    "    for i in range(0,2*train_size):\n",
    "        v_star[i]=v[i].x\n",
    "    w=evecs[0:train_size,0:2*train_size]@v_star\n",
    "\n",
    "    #opt obj\n",
    "    #obj = m.getObjective()\n",
    "    #print('nathan opt obj: ', obj.getValue())\n",
    "    \n",
    "    return w\n",
    "\n",
    "w_nathan_lambda_1=nathans_method(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e319ceb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#subroutine that computes phi(w) using binary search\n",
    "#input: weights. output: phi(w) and gradient\n",
    "def subroutine_binary_search(weights):\n",
    "\n",
    "    #compute D(w)\n",
    "    wSumHalfG=(G[:,0:train_size].dot(weights)).reshape(-1,1)\n",
    "    D2=2*wSumHalfG.dot(rowSumHalfG)/train_size#the second term in Dw\n",
    "    D1=wSumHalfG.dot(np.transpose(wSumHalfG))#the first term in Dw\n",
    "    for i in range(0,train_size):\n",
    "        D1=D1-arr_m[i]*weights[i]**2\n",
    "    Dw=-2*D1+D2+np.transpose(D2)-2*D3#Dw may not be PSD\n",
    "\n",
    "    #compute S\n",
    "    M=np.linalg.solve(C,Dw)#solve matrix M for CM=Dw\n",
    "    B=np.linalg.solve(C,np.transpose(M))#solve matrix B for CB=M^\\top\n",
    "\n",
    "    evals,evecs=scipy.linalg.eigh(B, eigvals_only=False)#symmetric QR\n",
    "    Q=np.copy(evecs)\n",
    "    delta=np.copy(evals)\n",
    "    S=np.linalg.solve(np.transpose(C),Q)\n",
    "    \n",
    "    #compute bw, epsilon\n",
    "    temp2=np.multiply(np.multiply(weights,weights),P_train)\n",
    "    bw=-G[:,0:train_size]@temp2\n",
    "    epsilon=np.transpose(S).dot(bw)\n",
    "    \n",
    "    if delta[0]*delta[-1]>0:\n",
    "        print('hessian is not indefinite')\n",
    "    #binary search algorithm\n",
    "    lambda_L=max(0,-min(delta))#ensure delta+lambda_L>=0\n",
    "    lambda_U=100\n",
    "    tol=0.000000001      \n",
    "    while lambda_U-lambda_L>=tol:\n",
    "        lambda_M=(lambda_U+lambda_L)/2\n",
    "        x_star=np.divide(-epsilon,delta+lambda_M)\n",
    "        if np.linalg.norm(x_star)>Gamma*(2**0.5):\n",
    "            lambda_L=lambda_M\n",
    "        else:\n",
    "            lambda_U=lambda_M \n",
    "    optimal_dual=lambda_U\n",
    "    x_star=np.divide(-epsilon,delta+optimal_dual) #recover opti primal variable from opti dual variable\n",
    "    \n",
    "    #get key values\n",
    "    obj = np.multiply(delta,x_star)@x_star/2+epsilon@x_star\n",
    "    obj_val = -obj #return phi(w) \n",
    "    q=S@x_star#worst case r()\n",
    "    \n",
    "    #compute gradient\n",
    "    grad=np.zeros(train_size)\n",
    "    for i in range(0,train_size):\n",
    "        temp1=np.transpose(G[:,i]).reshape(-1,1)\n",
    "        temp2=np.transpose(wSumHalfG)-weights[i]*np.transpose(temp1)-rowSumHalfG/train_size\n",
    "        Hessian=2*temp1.dot(temp2)\n",
    "        grad[i]=q.dot(Hessian.dot(np.transpose(q)))\n",
    "        grad[i]=grad[i]+2*weights[i]*P_train[i]*np.transpose(G[:,i]).dot(q)\n",
    "    \n",
    "    return obj_val,grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf34e3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute gradients using finite central diffs\n",
    "#input weights and objective value at this set of weights\n",
    "def finite_centra_diff(weights,obj_1):\n",
    "    finite_central_grad=np.zeros(train_size)\n",
    "    h=0.0001\n",
    "    for i in range(0,train_size):\n",
    "        #w_temp=np.copy(weights)\n",
    "        #w_temp[i]=w_temp[i]-h\n",
    "        #obj_1,_=subroutine_binary_search(w_temp)\n",
    "    \n",
    "        w_temp=np.copy(weights)\n",
    "        w_temp[i]=w_temp[i]+h\n",
    "        obj_2,_=subroutine_binary_search(w_temp)\n",
    "\n",
    "        finite_central_grad[i]=(obj_2-obj_1)/(h)\n",
    "    return finite_central_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599884a5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e8c650f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#subroutine that computes phi(w) using Ben-tal's method\n",
    "#input: weights. output: phi(w) and gradient\n",
    "def subroutine(weights):\n",
    "\n",
    "    #compute D(w)\n",
    "    wSumHalfG=(G[:,0:train_size].dot(weights)).reshape(-1,1)\n",
    "    D2=2*wSumHalfG.dot(rowSumHalfG)/train_size#the second term in Dw\n",
    "    D1=wSumHalfG.dot(np.transpose(wSumHalfG))#the first term in Dw\n",
    "    for i in range(0,train_size):\n",
    "        D1=D1-arr_m[i]*weights[i]**2\n",
    "    Dw=-2*D1+D2+np.transpose(D2)-2*D3#Dw may not be PSD\n",
    "\n",
    "    #compute S\n",
    "    M=np.linalg.solve(C,Dw)#solve matrix M for CM=Dw\n",
    "    B=np.linalg.solve(C,np.transpose(M))#solve matrix B for CB=M^\\top\n",
    "\n",
    "    evals,evecs=scipy.linalg.eigh(B, eigvals_only=False)#symmetric QR\n",
    "    Q=np.copy(evecs)\n",
    "    delta=np.copy(evals)\n",
    "    S=np.linalg.solve(np.transpose(C),Q)\n",
    "    \n",
    "    #compute bw, epsilon\n",
    "    temp2=np.multiply(np.multiply(weights,weights),P_train)\n",
    "    bw=-G[:,0:train_size]@temp2\n",
    "    epsilon=np.transpose(S).dot(bw)\n",
    "    \n",
    "    #build optimization model\n",
    "    m = Model()\n",
    "    m.Params.LogToConsole = 0#suppress Gurobipy printing\n",
    "    # Add variables\n",
    "    y=m.addMVar ( train_size*2, ub=100.0,lb=-100.0,name=\"y\" )\n",
    "    x=m.addMVar ( train_size*2, ub=100.0,lb=-100.0,name=\"x\" )    \n",
    "    # Set objective function\n",
    "    #m.Params.OptimalityTol=0.01\n",
    "    m.setObjective(epsilon@x+delta@y, GRB.MINIMIZE) \n",
    "    # Add constraints\n",
    "    m.addConstr(np.ones(train_size*2)@y<=Gamma**2)\n",
    "    m.addConstrs(x[i]@x[i]*0.5<=y[i] for i in range(train_size*2))\n",
    "    m.params.method=1#0 is primal simplex, 1 is dual simplex, 2 is barrier\n",
    "    #m.params.NonConvex=2\n",
    "    m.update()\n",
    "    m.optimize()\n",
    "    \n",
    "    #get key values\n",
    "    obj = m.getObjective()\n",
    "    obj_val = -obj.getValue() #return phi(w) \n",
    "    \n",
    "    x_star=np.zeros(2*train_size)\n",
    "    for i in range(0,2*train_size):\n",
    "        x_star[i]=x[i].x\n",
    "    q=S@x_star#worst case r()\n",
    "    \n",
    "    #compute gradient\n",
    "    grad=np.zeros(train_size)\n",
    "    for i in range(0,train_size):\n",
    "        temp1=np.transpose(G[:,i]).reshape(-1,1)\n",
    "        temp2=np.transpose(wSumHalfG)-weights[i]*np.transpose(temp1)-rowSumHalfG/train_size\n",
    "        Hessian=2*temp1.dot(temp2)\n",
    "        grad[i]=q.dot(Hessian.dot(np.transpose(q)))\n",
    "        grad[i]=grad[i]+2*weights[i]*P_train[i]*np.transpose(G[:,i]).dot(q)\n",
    "    \n",
    "    return obj_val,grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "467dfa27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check that bisection method and Ben-tal's method give the same gradient and objective value\n",
    "for k in range(0,50):\n",
    "    w_random=np.random.uniform(0,1,train_size)\n",
    "    w_random=w_random/sum(w_random)\n",
    "    obj_test1,grad_test1=subroutine(w_random)\n",
    "    obj_test2,grad_test2=subroutine_binary_search(w_random)\n",
    "    if abs(obj_test1-obj_test2)/obj_test1>0.005 or max(abs(np.divide(grad_test1-grad_test2,grad_test1)))>0.005:\n",
    "        print('iterate: ',k,' not match','abs(obj_test1-obj_test2)= ',abs(obj_test1-obj_test2))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90bb2640",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial objective value:  0.0118439509580203\n",
      "iter:  100 obj val:  0.011001011146267906 gt:  0.0037467531226543435 min grad:  0.01831075640807082 coord:  19\n",
      "iter:  200 obj val:  0.010861271233299967 gt:  0.0018410944857810661 min grad:  0.019916701924151067 coord:  21\n",
      "iter:  300 obj val:  0.010820294640463971 gt:  0.0011105912033937387 min grad:  0.02056087231781029 coord:  8\n",
      "iter:  400 obj val:  0.010804434132598541 gt:  0.0007148897421997355 min grad:  0.020929448573352338 coord:  22\n",
      "iter:  500 obj val:  0.010797131233530833 gt:  0.0005184320980270198 min grad:  0.021121158165591923 coord:  22\n",
      "iter:  600 obj val:  0.010793169546102748 gt:  0.0003873487300103239 min grad:  0.02124447370041585 coord:  19\n",
      "iter:  700 obj val:  0.010790822167503988 gt:  0.0003070228512695554 min grad:  0.021322468416846166 coord:  16\n",
      "iter:  800 obj val:  0.010789277988083932 gt:  0.0002565644299150731 min grad:  0.021372834875971147 coord:  6\n",
      "iter:  900 obj val:  0.010788198791717525 gt:  0.00021583775040343728 min grad:  0.02141074236763743 coord:  22\n",
      "iter:  1000 obj val:  0.010787440306097832 gt:  0.0001806745230113673 min grad:  0.02144467795523143 coord:  13\n",
      "runtime:  11.016414880752563\n"
     ]
    }
   ],
   "source": [
    "#main loop. Frank-Wolfe algorithm. Now use binary search\n",
    "w=w_nathan_lambda_1\n",
    "obj_val_old,grad=subroutine_binary_search(w) \n",
    "print('initial objective value: ',obj_val_old) \n",
    "#if Gamma>3:\n",
    "#    grad=finite_centra_diff(w,obj_val_old)\n",
    "\n",
    "k=1 \n",
    "L=Gamma*100#a guess on lipschitz constant of phi(w) \n",
    "maxIter=1000\n",
    "MSE_arr=np.zeros(maxIter+1) \n",
    "MSE_arr[0]=obj_val_old \n",
    "stepSize_arr=np.zeros(maxIter+1) \n",
    "step_size=1\n",
    "gt=1\n",
    "eta=0.1#parameter in Armijo rule\n",
    "tau=0.8#parameter in Armijo rule\n",
    "counter_small_steps=0\n",
    "while (k<=maxIter and counter_small_steps<100):\n",
    "\n",
    "    #compute descent direction\n",
    "    #if Gamma>3:#when Gamma>3, Danskin's Theorem no longer apply\n",
    "    #    grad=finite_centra_diff(w,obj_val_old)\n",
    "    ind=np.argmin(grad)\n",
    "    s_t=np.zeros(train_size)\n",
    "    s_t[ind]=1\n",
    "    descent_dir=s_t-w\n",
    "\n",
    "    #compute initial step size\n",
    "    gt=-grad@descent_dir\n",
    "    step_size=min(gt/(L*descent_dir@descent_dir),1)\n",
    "    \n",
    "    #compute obj val and grad for initial step size\n",
    "    obj_val_new,grad=subroutine_binary_search(w+step_size*descent_dir)\n",
    "       \n",
    "    #use Armijo rule for step size\n",
    "    counter_Armijo=0\n",
    "    while obj_val_old-obj_val_new<step_size*gt*eta and counter_Armijo<20:\n",
    "        step_size=step_size*tau\n",
    "        counter_Armijo+=1\n",
    "        obj_val_new,grad=subroutine_binary_search(w+step_size*descent_dir)\n",
    "        if counter_Armijo==20:\n",
    "            counter_small_steps+=1\n",
    "            if counter_small_steps==100:\n",
    "                print('terminated before maxIter')\n",
    "    \n",
    "    obj_val_old=obj_val_new\n",
    "    #store key outputs\n",
    "    MSE_arr[k]=obj_val_new\n",
    "    stepSize_arr[k]=step_size\n",
    "\n",
    "    #update weights\n",
    "    w=w+step_size*descent_dir\n",
    "    if (k % 100==0):\n",
    "        print('iter: ',k,'obj val: ',obj_val_new,'gt: ',gt,'min grad: ',min(grad),'coord: ',ind)\n",
    "    k=k+1\n",
    "\n",
    "#truncate dummy values\n",
    "total_iter=k\n",
    "MSE_arr=MSE_arr[0:k]\n",
    "stepSize_arr=stepSize_arr[0:k]\n",
    "print('runtime: ',time.time()-t) \n",
    "w_ours_binary_search=np.copy(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3b238dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.146427185010341"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_ours_binary_search@R_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be1a6c74",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finite central diff grad: \n",
      "[0.0225621  0.02260835 0.02145721 0.02203038 0.02145341 0.02144991\n",
      " 0.02144651 0.02146779 0.02144993 0.02204662 0.02145708 0.02213756\n",
      " 0.02146349 0.02147422 0.02146685 0.021457   0.02145996 0.02219739\n",
      " 0.02144476 0.02146312 0.02250346 0.021457   0.02145752 0.02145938\n",
      " 0.02145712 0.02146015 0.02146278 0.02161204 0.02145327 0.02144563]\n",
      "envelope theorem grad: \n",
      "[0.0225617  0.02260793 0.02145718 0.02203009 0.02145329 0.0214498\n",
      " 0.02144637 0.02146775 0.02144981 0.02204632 0.02145698 0.02213725\n",
      " 0.02146343 0.02147413 0.0214667  0.02145695 0.02145988 0.02219706\n",
      " 0.02144468 0.021463   0.02250307 0.02145688 0.02145739 0.02145934\n",
      " 0.0214571  0.02145998 0.02146267 0.02161181 0.02145316 0.02144546]\n",
      "percentage diff:  [1.77933246e-05 1.83414930e-05 1.07717195e-06 1.33045897e-05\n",
      " 5.43766134e-06 4.86014499e-06 6.49283102e-06 2.01436438e-06\n",
      " 5.70070780e-06 1.33961200e-05 4.35051545e-06 1.40173586e-05\n",
      " 2.64307050e-06 3.95915444e-06 6.75014038e-06 2.50209278e-06\n",
      " 3.72952826e-06 1.45001870e-05 3.63697779e-06 5.56582595e-06\n",
      " 1.71972680e-05 5.61025460e-06 5.86332124e-06 1.47266299e-06\n",
      " 8.81644492e-07 7.75368776e-06 4.75502785e-06 1.06978218e-05\n",
      " 5.35502342e-06 7.90396105e-06]\n"
     ]
    }
   ],
   "source": [
    "#check that gradient using Danskin's theorem and gradients using finite central diff are the same\n",
    "#this check is performed on bisection method\n",
    "w_t=np.copy(w)\n",
    "_,grad=subroutine_binary_search(w_t)\n",
    "\n",
    "finite_central_grad=np.zeros(train_size)\n",
    "h=0.001\n",
    "for i in range(0,train_size):\n",
    "    w_temp=np.copy(w_t)\n",
    "    w_temp[i]=w_temp[i]-h\n",
    "    obj_1,_=subroutine_binary_search(w_temp)\n",
    "    \n",
    "    w_temp=np.copy(w_t)\n",
    "    w_temp[i]=w_temp[i]+h\n",
    "    obj_2,_=subroutine_binary_search(w_temp)\n",
    "    #approx grad\n",
    "    finite_central_grad[i]=(obj_2-obj_1)/(2*h)\n",
    "print('finite central diff grad: ')\n",
    "print(finite_central_grad)\n",
    "print('envelope theorem grad: ')\n",
    "print(grad)\n",
    "print('percentage diff: ',(finite_central_grad-grad)/grad)\n",
    "\n",
    "finite_central_grad_2=np.zeros(train_size)\n",
    "for i in range(0,train_size):\n",
    "    w_temp=np.copy(w_t)\n",
    "    w_temp[i]=w_temp[i]-h\n",
    "    obj_1,_=subroutine(w_temp)\n",
    "    \n",
    "    w_temp=np.copy(w_t)\n",
    "    w_temp[i]=w_temp[i]+h\n",
    "    obj_2,_=subroutine(w_temp)\n",
    "    #approx grad\n",
    "    finite_central_grad_2[i]=(obj_2-obj_1)/(2*h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1613daae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.05789517e-04,  6.78174006e-04,  1.62879834e-02,  4.36857144e-04,\n",
       "        5.38330047e-04, -2.68297734e-04,  1.05389183e-02, -2.41389844e-02,\n",
       "       -3.09522304e-03,  5.49238601e-04,  3.01828661e-02,  4.01025480e-04,\n",
       "       -2.02688143e-02,  3.32703847e-02, -2.58592960e-03,  1.30480969e-03,\n",
       "        6.18401233e-03, -1.09093657e-01, -1.20470801e-03, -2.07493148e-03,\n",
       "        3.19553174e-03,  2.13617155e-05,  1.89575492e-03,  4.53663521e-04,\n",
       "       -5.35034598e-05,  7.93560925e-04,  1.40323421e-02,  7.28487939e-03,\n",
       "        1.46905236e-03, -3.93894569e-03])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(finite_central_grad_2-finite_central_grad)/finite_central_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27a21951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial objective value:  0.011843755126783592\n",
      "iter:  100 obj val:  0.011002743709307022 gt:  0.00374217608457675 min grad:  0.01831418893309774 coord:  19\n",
      "iter:  200 obj val:  0.010861908372729595 gt:  0.0018493578900338169 min grad:  0.019921793739592665 coord:  23\n",
      "iter:  300 obj val:  0.010821524406870417 gt:  0.0011298135323369856 min grad:  0.02054097626883627 coord:  26\n",
      "iter:  400 obj val:  0.010807899874916059 gt:  0.0008184345946146022 min grad:  0.020826057757436102 coord:  16\n",
      "iter:  500 obj val:  0.010800689946345253 gt:  0.0006232676648849945 min grad:  0.021020441635815647 coord:  2\n",
      "iter:  600 obj val:  0.010796367040926815 gt:  0.000496813903972243 min grad:  0.0211290672959421 coord:  26\n",
      "iter:  700 obj val:  0.010793864883809389 gt:  0.00042565746824868925 min grad:  0.021208114183506998 coord:  16\n",
      "iter:  800 obj val:  0.01079251722775131 gt:  0.0003748486794344155 min grad:  0.021254785455782826 coord:  18\n",
      "terminated before maxIter\n",
      "runtime:  682.4900619983673\n"
     ]
    }
   ],
   "source": [
    "#main loop. Frank-Wolfe algorithm. Using Ben-tal's subroutine\n",
    "w=w_nathan_lambda_1\n",
    "obj_val_old,grad=subroutine(w) \n",
    "print('initial objective value: ',obj_val_old) \n",
    "#if Gamma>3:\n",
    "#    grad=finite_centra_diff(w,obj_val_old)\n",
    "\n",
    "k=1 \n",
    "L=Gamma*100#a guess on lipschitz constant of phi(w) \n",
    "maxIter=1000\n",
    "MSE_arr=np.zeros(maxIter+1) \n",
    "MSE_arr[0]=obj_val_old \n",
    "stepSize_arr=np.zeros(maxIter+1) \n",
    "step_size=1\n",
    "gt=1\n",
    "eta=0.1#parameter in Armijo rule\n",
    "tau=0.8#parameter in Armijo rule\n",
    "counter_small_steps=0\n",
    "while (k<=maxIter and counter_small_steps<100):\n",
    "\n",
    "    #compute descent direction\n",
    "    #if Gamma>3:#when Gamma>3, Danskin's Theorem no longer apply\n",
    "    #    grad=finite_centra_diff(w,obj_val_old)\n",
    "    ind=np.argmin(grad)\n",
    "    s_t=np.zeros(train_size)\n",
    "    s_t[ind]=1\n",
    "    descent_dir=s_t-w\n",
    "\n",
    "    #compute initial step size\n",
    "    gt=-grad@descent_dir\n",
    "    step_size=min(gt/(L*descent_dir@descent_dir),1)\n",
    "    \n",
    "    #compute obj val and grad for initial step size\n",
    "    obj_val_new,grad=subroutine(w+step_size*descent_dir)\n",
    "       \n",
    "    #use Armijo rule for step size\n",
    "    counter_Armijo=0\n",
    "    while obj_val_old-obj_val_new<step_size*gt*eta and counter_Armijo<20:\n",
    "        step_size=step_size*tau\n",
    "        counter_Armijo+=1\n",
    "        obj_val_new,grad=subroutine(w+step_size*descent_dir)\n",
    "        if counter_Armijo==20:\n",
    "            counter_small_steps+=1\n",
    "            if counter_small_steps==100:\n",
    "                print('terminated before maxIter')\n",
    "    \n",
    "    obj_val_old=obj_val_new\n",
    "    #store key outputs\n",
    "    MSE_arr[k]=obj_val_new\n",
    "    stepSize_arr[k]=step_size\n",
    "\n",
    "    #update weights\n",
    "    w=w+step_size*descent_dir\n",
    "    if (k % 100==0):\n",
    "        print('iter: ',k,'obj val: ',obj_val_new,'gt: ',gt,'min grad: ',min(grad),'coord: ',ind)\n",
    "    k=k+1\n",
    "\n",
    "#truncate dummy values\n",
    "total_iter=k\n",
    "MSE_arr=MSE_arr[0:k]\n",
    "stepSize_arr=stepSize_arr[0:k]\n",
    "print('runtime: ',time.time()-t) \n",
    "w_ours=np.copy(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2304eff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "approx grad: \n",
      "[ 0.02263274  0.02299595  0.01948351  0.02416172  0.01987304  0.02161251\n",
      "  0.02270028  0.02160432  0.02135592 -0.03876797  0.01393507  0.02021285\n",
      "  0.02154924  0.02105198  0.02146526  0.02157159  0.02161349  0.02237629\n",
      "  0.02167469  0.02144934  0.01994088  0.01961288  0.02085066  0.0191808\n",
      "  0.02189582  0.01866441  0.12778892  0.02504859  0.02155588  0.02188996]\n",
      "envelope theorem grad: \n",
      "[0.02255249 0.02259855 0.02145725 0.02202252 0.02144919 0.02144604\n",
      " 0.02145287 0.02146609 0.02145589 0.02203871 0.02146225 0.02212939\n",
      " 0.02146742 0.02147095 0.02146191 0.02146078 0.02146474 0.02218903\n",
      " 0.0214495  0.021469   0.02249407 0.02146292 0.02146357 0.02145835\n",
      " 0.02145823 0.02145472 0.02146822 0.02160528 0.02145908 0.02144016]\n",
      "percentage diff:  [ 3.55823903e-03  1.75853365e-02 -9.19847360e-02  9.71368178e-02\n",
      " -7.34829355e-02  7.76231080e-03  5.81464604e-02  6.43959403e-03\n",
      " -4.65970288e-03 -2.75908503e+00 -3.50717323e-01 -8.66059117e-02\n",
      "  3.81134110e-03 -1.95133384e-02  1.55982707e-04  5.16366007e-03\n",
      "  6.92999532e-03  8.43957310e-03  1.04985587e-02 -9.15582373e-04\n",
      " -1.13504796e-01 -8.61969235e-02 -2.85558088e-02 -1.06138095e-01\n",
      "  2.03926072e-02 -1.30055658e-01  4.95246859e+00  1.59373812e-01\n",
      "  4.51087750e-03  2.09796489e-02]\n"
     ]
    }
   ],
   "source": [
    "#check that gradient using Danskin's theorem and gradients using finite central diff are the same\n",
    "#this check is performed on Ben-tal's method\n",
    "w_t=np.copy(w_ours_binary_search)\n",
    "obj_1,grad=subroutine(w_t)\n",
    "approx_grad_arr=np.zeros(train_size)\n",
    "h=0.0001\n",
    "for i in range(0,train_size):\n",
    "    w_temp=np.copy(w_t)\n",
    "    w_temp[i]=w_temp[i]-h\n",
    "    obj_1,_=subroutine(w_temp)\n",
    "    \n",
    "    w_temp=np.copy(w_t)\n",
    "    w_temp[i]=w_temp[i]+h\n",
    "    obj_2,_=subroutine(w_temp)\n",
    "    #approx grad\n",
    "    approx_grad_arr[i]=(obj_2-obj_1)/(2*h)\n",
    "print('approx grad: ')\n",
    "print(approx_grad_arr)\n",
    "print('envelope theorem grad: ')\n",
    "print(grad)\n",
    "print('percentage diff: ',(approx_grad_arr-grad)/grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "baf4621e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.02899978, 0.02911225, 0.02879945, 0.02794006, 0.02646201,\n",
       "       0.0265204 , 0.04721514, 0.02747709, 0.04466272, 0.02796795,\n",
       "       0.04050039, 0.02812822, 0.03595433, 0.02668809, 0.0263085 ,\n",
       "       0.03540741, 0.03885907, 0.02823775, 0.0386991 , 0.04404613,\n",
       "       0.0288635 , 0.04436217, 0.04514516, 0.0278781 , 0.03011012,\n",
       "       0.02641896, 0.04178464, 0.02728416, 0.04371561, 0.02645177])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_ours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ed5edf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.98091319, 0.98091319, 1.00834158, 0.98091319, 1.00885852,\n",
       "       1.00935301, 1.0030188 , 1.00739759, 1.00309254, 0.98091319,\n",
       "       1.00442707, 0.98091319, 1.00457294, 1.01073235, 1.01118399,\n",
       "       1.00639287, 1.00491707, 0.98091319, 1.00485703, 1.005326  ,\n",
       "       0.98091319, 1.00356584, 1.00416227, 1.00859363, 1.0059118 ,\n",
       "       1.00509725, 1.00478736, 0.98091319, 1.00396055, 1.00307445])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#4. show that at termination of Frank Wolfe, we get the same weights by using bisection or Ben-tal's method.\n",
    "w_ours_binary_search/w_ours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1490e39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
